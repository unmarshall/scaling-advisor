// SPDX-FileCopyrightText: 2025 SAP SE or an SAP affiliate company and Gardener contributors
//
// SPDX-License-Identifier: Apache-2.0

package service

import (
	"context"
	"fmt"
	"io"
	"sync/atomic"
	"time"

	commonconstants "github.com/gardener/scaling-advisor/api/common/constants"
	commontypes "github.com/gardener/scaling-advisor/api/common/types"
	sacorev1alpha1 "github.com/gardener/scaling-advisor/api/core/v1alpha1"
	mkapi "github.com/gardener/scaling-advisor/api/minkapi"
	corev1 "k8s.io/api/core/v1"
	nodev1 "k8s.io/api/node/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/events"
)

const (
	// ProgramName is the program name for the scaling advisor service.
	ProgramName = "scadsvc"
)

// ActivityStatus represents the operational status of an activity.
type ActivityStatus string

const (
	// ActivityStatusPending indicates the activity is pending execution.
	ActivityStatusPending ActivityStatus = "Pending"
	// ActivityStatusRunning indicates the activity is currently running.
	ActivityStatusRunning ActivityStatus = "Running"
	// ActivityStatusSuccess indicates the activity completed successfully.
	ActivityStatusSuccess ActivityStatus = metav1.StatusSuccess
	// ActivityStatusFailure indicates the activity failed.
	ActivityStatusFailure ActivityStatus = metav1.StatusFailure
)

// ScalingAdviceRequest encapsulates the request parameters for generating scaling advice.
type ScalingAdviceRequest struct {
	// Snapshot is the snapshot of the resources in the cluster at the time of the request.
	Snapshot *ClusterSnapshot
	// Feedback captures feedback from the consumer of the scaling advice, which can be used to improve future scaling advice generation.
	Feedback *sacorev1alpha1.ClusterScalingFeedback
	// Constraint represents the constraints using which the scaling advice is generated.
	Constraint *sacorev1alpha1.ClusterScalingConstraint
	ScalingAdviceRequestRef
	// EnableDiagnostics indicates whether diagnostics should be enabled for the scaling advice generation.
	// If it is enabled then it will populate ScalingAdviceResponse.ScalingAdvice.Status.Diagnostic field.
	EnableDiagnostics bool
}

// ScalingAdviceRequestRef is the unique reference to a scaling advice request.
type ScalingAdviceRequestRef struct {
	// ID is the Request unique identifier for which this response is generated.
	ID string
	// CorrelationID is the correlation identifier for the request. It can be used to correlate a response with a request.
	CorrelationID string
}

// ScalingAdviceResponse encapsulates the response from the scaling advisor service.
type ScalingAdviceResponse struct {
	// ScalingAdvice contains the scaling advice generated by the scaling advisor service.
	ScalingAdvice *sacorev1alpha1.ClusterScalingAdvice
	// Diagnostic provides diagnostics information for the scaling advice.
	// This is only set by the scaling advisor service if ScalingAdviceRequest.EnableDiagnostics is set to true.
	Diagnostics *sacorev1alpha1.ScalingAdviceDiagnostic
	// RequestRef encapsulates the unique reference to a request for which this response is produced.
	RequestRef ScalingAdviceRequestRef
	// Message is a human-readable message providing additional context about the response.
	Message string
}

// ScalingAdvisorServiceConfig holds the configuration for the scaling advisor service.
type ScalingAdvisorServiceConfig struct {
	// CloudProvider is the cloud provider for which the scaling advisor service is initialized.
	CloudProvider commontypes.CloudProvider
	// LogBaseDir is the base directory for storing log files used by the scaling advisor service.
	LogBaseDir string
	commontypes.ServerConfig
	// MinKAPIConfig holds the configuration for the MinKAPI server used by the scaling advisor service.
	MinKAPIConfig mkapi.Config
	commontypes.QPSBurst
	// MaxParallelSimulations is the maximum number of parallel simulations that can be run by the scaling advisor service.
	MaxParallelSimulations int
}

// ScalingAdviceResponseFn is a callback function which is invoked by the scaling advisor service when generating scaling advice.
// The callback maybe invoked zero or more times during the generation of scaling advice with callbackType representing the type of change in the scaling advice.
type ScalingAdviceResponseFn func(response ScalingAdviceResponse) error

// ScalingAdviceResult represents a single result emitted by the ScalingAdvisorService
// during the generation of scaling advice.
//
// Each result will contain either a Response or an Err:
//
//   - If Response is non-nil, it contains one piece of scaling advice.
//   - If Err is non-nil, it represents a terminal error that occurred during
//     advice generation. No further results will be sent after an error result.
//
// The caller should treat the channel of ScalingAdviceResult as a stream of
// results that may end either because the service has finished generating
// responses or because an error was encountered. The stream ends when the channel
// is closed.
//
// Usage:
//
//	results := svc.GenerateAdvice(ctx, req)
//	for r := range results {
//	    if r.Err != nil {
//	        log.Printf("advice generation failed: %v", r.Err)
//	        break
//	    }
//	    process(r.Response)
//	}
//
// If the provided context is canceled, the service should stop generation and
// close the channel.
type ScalingAdviceResult struct {
	// Response contains a piece of scaling advice generated by the service.
	// It is non-nil only when Err is nil.
	Response *ScalingAdviceResponse

	// Err contains a fatal error that occurred during generation.
	// It is non-nil only when Response is nil.
	Err error
}

// ScalingAdvisorService is the high-level facade for the scaling advisor service.
type ScalingAdvisorService interface {
	commontypes.Service
	// GenerateAdvice begins generating scaling advice for the given request.
	//
	// It returns a channel of ScalingAdviceResult values. The channel will be closed
	// when advice generation is completed or a fatal error has occurred.
	//
	// The caller must consume all events from the channel until it is closed to
	// avoid leaking goroutines inside the service implementation.
	//
	// The provided context can be used to cancel generation prematurely. In this
	// case, the channel will be closed without further events.
	GenerateAdvice(ctx context.Context, req ScalingAdviceRequest) <-chan ScalingAdviceResult
}

// App represents an application process that wraps a ScalingAdvisorService, an application context and application cancel func.
// `main` entry-point functions that embed scadsvc are expected to construct a new App instance via cli.LaunchApp and shutdown applications via cli.ShutdownApp
type App struct {
	// Service is the scaling advisor service instance.
	Service ScalingAdvisorService
	// Ctx is the application context.
	Ctx context.Context
	// Cancel is the context cancellation function.
	Cancel context.CancelFunc
}

// SchedulerLaunchParams holds the parameters required to launch a kube-scheduler instance.
type SchedulerLaunchParams struct {
	// EventSink is the event sink used to send events from the kube-scheduler.
	EventSink events.EventSink
	commontypes.ClientFacades
}

// SchedulerLauncher defines the interface for launching a kube-scheduler instance.
// There will be a limited number of kube-scheduler instances that can be launched at a time.
type SchedulerLauncher interface {
	// Launch launches and runs an embedded scheduler instance asynchronously.
	// If the limit of running schedulers is reached, it will block.
	// An error is returned if the scheduler fails to start.
	Launch(ctx context.Context, params *SchedulerLaunchParams) (SchedulerHandle, error)
}

// SchedulerHandle defines the interface for managing a kube-scheduler instance.
type SchedulerHandle interface {
	io.Closer
	// GetParams returns the parameters used to launch the scheduler instance.
	GetParams() SchedulerLaunchParams
}

// ClusterSnapshot represents a snapshot of the cluster at a specific time and encapsulates the scheduling relevant information required by the kube-scheduler.
// Pods inside the ClusterSnapshot should not have SchedulingGates - these should be filtered out by creator of the ClusterSnapshot.
type ClusterSnapshot struct {
	// Pods are the pods that are present in the cluster.
	Pods []PodInfo
	// Nodes are the nodes that are present in the cluster.
	Nodes []NodeInfo
	// PriorityClasses are the priority classes that are present in the cluster.
	PriorityClasses []schedulingv1.PriorityClass
	// RuntimeClasses are the runtime classes that are present in the cluster.
	RuntimeClasses []nodev1.RuntimeClass
}

// GetUnscheduledPods returns all pods in the cluster snapshot that are not scheduled to any node.
func (c *ClusterSnapshot) GetUnscheduledPods() []PodInfo {
	var unscheduledPods []PodInfo
	for _, pod := range c.Pods {
		if pod.NodeName == "" {
			unscheduledPods = append(unscheduledPods, pod)
		}
	}
	return unscheduledPods
}

// GetNodeCountByPlacement returns a map of node placements to their respective node counts in the cluster.
func (c *ClusterSnapshot) GetNodeCountByPlacement() (map[sacorev1alpha1.NodePlacement]int32, error) {
	nodeCountByPlacement := make(map[sacorev1alpha1.NodePlacement]int32)
	for _, nodeInfo := range c.Nodes {
		p, err := getNodePlacement(nodeInfo)
		if err != nil {
			return nil, err
		}
		nodeCountByPlacement[p]++
	}
	return nodeCountByPlacement, nil
}

// getNodePlacement extracts the node placement information from a NodeInfo using labels.
func getNodePlacement(nodeInfo NodeInfo) (placement sacorev1alpha1.NodePlacement, err error) {
	nodeTemplateName, ok := nodeInfo.Labels[commonconstants.LabelNodeTemplateName]
	if !ok {
		err = fmt.Errorf("%w: %s", ErrMissingRequiredLabel, commonconstants.LabelNodeTemplateName)
		return
	}
	nodePoolName, ok := nodeInfo.Labels[commonconstants.LabelNodePoolName]
	if !ok {
		err = fmt.Errorf("%w: %s", ErrMissingRequiredLabel, commonconstants.LabelNodePoolName)
		return
	}
	region, ok := nodeInfo.Labels[corev1.LabelTopologyRegion]
	if !ok {
		err = fmt.Errorf("%w: %s", ErrMissingRequiredLabel, corev1.LabelTopologyRegion)
		return
	}
	az, ok := nodeInfo.Labels[corev1.LabelTopologyZone]
	if !ok {
		err = fmt.Errorf("%w: %s", ErrMissingRequiredLabel, corev1.LabelTopologyZone)
		return
	}
	placement = sacorev1alpha1.NodePlacement{
		NodePoolName:     nodePoolName,
		NodeTemplateName: nodeTemplateName,
		InstanceType:     nodeInfo.InstanceType,
		Region:           region,
		AvailabilityZone: az,
	}
	return
}

// PodInfo contains the minimum set of information about corev1.Pod that will be required by the kube-scheduler.
// NOTES:
//  1. PodSchedulingGates should not be not part of PodInfo. It is expected that pods having scheduling gates will be filtered out before setting up simulation runs.
//  2. Consider including PodSpec.Resources in future when it graduates to beta/GA.
type PodInfo struct {
	ResourceMeta
	// AggregatedRequests is an aggregated resource requests for all containers of the Pod.
	AggregatedRequests map[corev1.ResourceName]int64 `json:"aggregatedRequests,omitempty"`
	// Volumes are the volumes that are attached to the Pod.
	Volumes []corev1.Volume `json:"volumes,omitempty"`
	// NodeSelector is the node selector for the Pod.
	NodeSelector map[string]string `json:"nodeSelector,omitempty"`
	// NodeName is the name of the node where the Pod is scheduled.
	NodeName string `json:"nodeName,omitempty"`
	// Affinity is the affinity rules for the Pod.
	Affinity *corev1.Affinity `json:"affinity,omitempty"`
	// SchedulerName is the name of the scheduler that should be used to schedule the Pod.
	SchedulerName string `json:"schedulerName,omitempty"`
	// Tolerations are the tolerations for the Pod.
	Tolerations []corev1.Toleration `json:"tolerations,omitempty"`
	// PriorityClassName is the name of the priority class that should be used to schedule the Pod.
	PriorityClassName         string                            `json:"priorityClassName,omitempty"`
	Priority                  *int32                            `json:"priority,omitempty"`
	PreemptionPolicy          *corev1.PreemptionPolicy          `json:"preemptionPolicy,omitempty"`
	RuntimeClassName          *string                           `json:"runtimeClassName,omitempty"`
	Overhead                  map[corev1.ResourceName]int64     `json:"overhead,omitempty"`
	TopologySpreadConstraints []corev1.TopologySpreadConstraint `json:"topologySpreadConstraints,omitempty"`
	ResourceClaims            []corev1.PodResourceClaim         `json:"resourceClaims,omitempty"`
}

// GetResourceInfo returns the resource information for the pod.
func (p *PodInfo) GetResourceInfo() PodResourceInfo {
	return PodResourceInfo{
		UID:                p.UID,
		NamespacedName:     p.NamespacedName,
		AggregatedRequests: p.AggregatedRequests,
	}
}

// NodeInfo contains the minimum set of information about corev1.Node that will be required by the kube-scheduler.
type NodeInfo struct {
	// Capacity is the total resource capacity of the node.
	Capacity map[corev1.ResourceName]int64 `json:"capacity,omitempty"`
	// Allocatable is the allocatable resource capacity of the node.
	Allocatable map[corev1.ResourceName]int64 `json:"allocatable,omitempty"`
	// CSIDriverVolumeMaximums is a map of CSI driver names to the maximum number of unique volumes managed by the
	// CSI driver that can be used on a node.
	CSIDriverVolumeMaximums map[string]int32 `json:"csiDriverVolumeMaximums,omitempty"`
	// InstanceType is the instance type for the Node.
	InstanceType string `json:"instanceType"`
	ResourceMeta
	// Taints are the node's taints.
	Taints []corev1.Taint `json:"taints,omitempty"`
	// Conditions are the node's conditions.
	Conditions []corev1.NodeCondition `json:"conditions,omitempty"`
	// Unschedulable indicates whether the node is unschedulable.
	Unschedulable bool `json:"unschedulable"`
}

// GetResourceInfo returns the resource information for the node.
func (n *NodeInfo) GetResourceInfo() NodeResourceInfo {
	return NodeResourceInfo{
		Name:         n.Name,
		InstanceType: n.InstanceType,
		Capacity:     n.Capacity,
		Allocatable:  n.Allocatable,
	}
}

// ResourceMeta contains common metadata fields for Kubernetes resources.
type ResourceMeta struct {
	// UID is the unique identifier for the resource.
	UID types.UID `json:"uid"`
	types.NamespacedName
	// Labels are the labels associated with the resource.
	Labels map[string]string `json:"labels,omitempty"`
	// Annotations are the annotations associated with the resource.
	Annotations map[string]string `json:"annotations,omitempty"`
	// DeletionTimestamp is the timestamp when the resource deletion was triggered.
	DeletionTimestamp *metav1.Time `json:"deletionTimestamp,omitempty"`
	// OwnerReferences are the owner references associated with the resource.
	OwnerReferences []metav1.OwnerReference `json:"ownerReferences,omitempty"`
}

// InstancePriceInfo contains pricing and specification information for a cloud instance type.
type InstancePriceInfo struct {
	// InstanceType is the name of the instance type.
	InstanceType string `json:"instanceType"`
	// Region is the cloud region where the instance is available.
	Region string `json:"region"`
	// OS is the operating system for the instance type.
	OS string `json:"os"`
	// Memory is the amount of memory in GB for the instance type.
	Memory int64 `json:"memory"`
	// GPUMemory is the amount of GPU memory in GB for the instance type.
	GPUMemory int64 `json:"GPUMemory"`
	// HourlyPrice is the hourly cost for the instance type.
	HourlyPrice float64 `json:"hourlyPrice"`
	// GPU is the number of GPUs for the instance type.
	GPU int32 `json:"GPU"`
	// VCPU is the number of virtual CPUs for the instance type.
	VCPU int32 `json:"VCPU"`
}

// PriceKey represents the key for a instance type price within a cloud provider.
type PriceKey struct {
	// Name is the instance type name.
	Name string
	// Region is the cloud region.
	Region string
}

// InstancePricingAccess defines an interface for accessing instance pricing information.
type InstancePricingAccess interface {
	// GetInfo gets the InstancePriceInfo (whicn includes price) for the given region and instance type.
	// TODO: should we also pass OS name here ? if so, we need to need to change ClusterScalingConstraint.
	GetInfo(region, instanceTypeName string) (InstancePriceInfo, error)
}

// GetProviderInstancePricingAccessFunc is a factory function for creating InstancePricingAccess implementations.
type GetProviderInstancePricingAccessFunc func(provider commontypes.CloudProvider, instanceTypeInfoPath string) (InstancePricingAccess, error)

// NodeScorer defines an interface for computing node scores for scaling decisions.
type NodeScorer interface {
	// Compute computes the node score given the NodeScorerArgs. On failure, it must return an error with the sentinel error api.ErrComputeNodeScore
	Compute(args NodeScorerArgs) (NodeScore, error)
}

// NodeScorerArgs contains arguments for node scoring computation.
type NodeScorerArgs struct {
	// ID that must be given to the NodeScore produced by the NodeScorer
	ID string
	// Placement represents the placement information for the Node.
	Placement sacorev1alpha1.NodePlacement
	// ScaledAssignment represents the assignment of the scaled Node for the current run.
	ScaledAssignment *NodePodAssignment
	// OtherAssignments represent the assignment of unscheduled Pods to either an existing Node which is part of the ClusterSnapshot
	// or it is a winning simulated Node from a previous run.
	OtherAssignments []NodePodAssignment
	// UnscheduledPods is the slice of unscheduled pods that remain unscheduled after simulation is completed.
	UnscheduledPods []types.NamespacedName
}

// NodeScore represents the scoring result for a node in scaling simulations.
type NodeScore struct {
	ScaledNodeResource NodeResourceInfo
	// Placement represents the placement information for the Node.
	Placement sacorev1alpha1.NodePlacement
	// ID uniquely identifies this NodeScore
	ID              string
	UnscheduledPods []types.NamespacedName
	// Value is the score value for this Node.
	Value int
}

// GetWeightsFunc is a function type for retrieving resource weights for scoring.
type GetWeightsFunc func(instanceType string) (map[corev1.ResourceName]float64, error)

// GetNodeScorer is a factory function for creating NodeScorer implementations.
type GetNodeScorer func(scoringStrategy commontypes.NodeScoringStrategy, instanceTypeInfoAccess InstancePricingAccess, weightsFn GetWeightsFunc) (NodeScorer, error)

// GetNodeScoreSelector is a factory function for creating NodeScoreSelector implementations.
type GetNodeScoreSelector func(scoringStrategy commontypes.NodeScoringStrategy) (NodeScoreSelector, error)

// PodResourceInfo contains resource information for a pod used in scoring calculations.
type PodResourceInfo struct {
	// AggregatedRequests is an aggregated resource requests for all containers of the Pod.
	AggregatedRequests map[corev1.ResourceName]int64
	// NamespacedName contains the namespace and name of the pod.
	types.NamespacedName
	// UID is the unique identifier for the pod.
	UID types.UID
}

// NodeResourceInfo represents the subset of NodeInfo such that NodeScorer can compute an effective NodeScore.
// TODO think of a better name.
type NodeResourceInfo struct {
	// Capacity is the total resource capacity of the node.
	Capacity map[corev1.ResourceName]int64
	// Allocatable is the allocatable resource capacity of the node.
	Allocatable map[corev1.ResourceName]int64
	// Name is the node name.
	Name string
	// InstanceType is the cloud instance type of the node.
	InstanceType string
}

// NodePodAssignment represents the assignment of pods to a node for simulation purposes.
type NodePodAssignment struct {
	// Node contains the resource information for the node.
	Node NodeResourceInfo
	// ScheduledPods contains the list of pods scheduled to this node.
	ScheduledPods []PodResourceInfo
}

// NodeScoreSelector selects the winning NodeScore amongst the NodeScores of a given simulation pass and returns the pointer to the same.
// If there is no winning node score amongst the group, then it returns nil.
type NodeScoreSelector func(groupNodeScores []NodeScore, weightsFn GetWeightsFunc, pricing InstancePricingAccess) (winningNodeScore *NodeScore, err error)

// SimulatorStrategy represents a simulator strategy variant.
// +enum
type SimulatorStrategy string

const (
	SimulatorStrategyScaleOnePerGroup  SimulatorStrategy = "scale-one-per-group"
	SimulatorStrategyScaleManyPerGroup SimulatorStrategy = "scale-many-per-group"
)

/*
	P1: Az-a, Az-b, Az-c
	NT: NT1

	PlanGenerator:
		* Selects a Simulator based on SimulationStrategy
		> Creation of SimulationGroup(s) based on NodePool(s), NodeTemplate(s) and AZ(s) combinations is a responsibility of the Simulator.
		* Invokes Simulator.Run which should return ScaleOutPlan.

	Simulator.Run:
		* Calls CreateSimulationGroups
			* Type 1: Creates a SimulationGroup having Simulation(s) for each combination of 1 NT + 1 NP + 1 AZ
			* Type 2: Creates a SimulationGroup having a single Simulation for multiple NTs + multiple NPs + multiple AZs
			* The SimulationGroups are ordered by priority of NodePool and NodeTemplate for Type1 or Type2.
		* Increments the pass counter and calls runPass on the SimulationGroups. This is done until one of the following happens:
			* The context is cancelled or timeout occurs.
			* There is no pass winner NodeScore(s) from a SimulationGroup in the current pass.
			* There are no leftover unscheduled pods.
		* RunPass of Type1:
			* Iterate over the SimulationGroups based on priority.
			* Call Simulator.RunGroup which executes each simulation in the group concurrently.
				* Simulation.Run:
					* Scales a single node for a combination of NT + NP + AZ.
					* Runs a scheduler and gives pod-to-node assignment for the scaled node and existing node(s) and leftover unscheduled pods.
				* For each simulation, gets the `SimulationResult`.
					* If there are at least one Pod scheduled in this pass then convert it into NodeScoreArgs and computes the NodeScoreResult.
					* If there are no pods scheduled in this pass then there is no need to compute a NodeScoreResult. This simulation does not participate further in winning NodeScoreResult selection.
				* If there are no computed NodeScoreResult's for the SimulationGroup, then it will consider the next group.
				* Selects the winning NodeScoreResult amongst all computed NodeScoreResult's for the SimulationGroup.
				* If there are no winning NodeScoreResult's, then it will consider the next group.
				* If there is a winning NodeScoreResult:
					* Get the unscheduled pods from the winning NodeScoreResult. If there are no unscheduled pods, then constructs the ScaleOutPlan and returns the same to the PlanGenerator.
					* If there are leftover unscheduled pods, it will re-run the simulations for the same group. In each such run it may choose a different winning NodeScoreResult.
		* RunPass of Type2:
			* Iterate over the SimulationGroups based on priority.
			* Call Simulator.RunGroup which executes the single simulation in the group.
				* Calls Simulation.Run:
					* Scales one Node per unique combination of NT + NP + AZ for all NTs + NPs + AZs associated with the SimulationGroup.
					* Runs a scheduler and gives pod-to-node assignments for all the scaled nodes and existing node(s) and leftover unscheduled pods.
				* For the simulation, gets the `SimulationResult`.
					* If there are no leftover unscheduled pods, constructs the ScaleOutPlan and returns the same to the PlanGenerator.
					* If there are leftover unscheduled pods, it will re-run the simulation for the same group.
						* If there are no pods scheduled in this pass then it will consider the next group.
						* If there is at least one pod scheduled in this pass, it will continue re-running the simulation for the same group.


	There are 2 kinds of Simulations
 	1. Associated with a 1 NT + 1 NP + 1 AZ. Scales a single node for this combination. Runs a scheduler and gives pod-to-node assignment for the scaled node and existing node(s) and leftover unscheduled pods.
	2. Associated with multiple NTs + multiple NPs + multiple AZs. Scales one Node per unique combination. Runs a scheduler and gives pod-to-node assignments for all the scaled nodes and existing node(s) and leftover unscheduled pods.

	There are 2 kinds of Simulators
	1. Creates a SimulationGroup holding multiple independent simulations of kind 1 above. Executes each simulation concurrently. Computes the node scores for each simulation and selects the winning node score amongst the group.
		1. If there are no winning node scores, then it will consider the next group. (If there are no pods scheduled in this run, then there will be no winning node score)
		2. If there is a winning node score, it will re-run the simulation for the same group.
	2. Creates a SimulationGroup holding a single simulation of kind 2 above. Executes the simulation.
		1. If there are no leftover unscheduled pods, it constructs node scores for the simulation run results of all simulation groups run so far and returns the same to the PlanGenerator.
		2. If there are leftover unscheduled pods, it will re-run the simulation for the same group.
			1. There are no further pod-to-node assignments possible, then it will consider the next group.
			2. There are further pod-to-node assignments possible, it will continue re-running the simulation for the same group.


*/

// Simulation represents an activity that performs valid unscheduled pod to ready node assignments on a minkapi View.
// A simulation implementation may use a k8s scheduler - either embedded or external to do this, or it may form a SAT/MIP model
// from the pod/node data and run a tool that solves the model.
type Simulation interface {
	// Name returns the logical simulation name
	Name() string
	// ActivityStatus returns the current ActivityStatus of the simulation
	ActivityStatus() ActivityStatus
	// NodePool returns the target node pool against which the simulation should be run
	NodePool() *sacorev1alpha1.NodePool
	// NodeTemplate returns the target node template against which the simulation should be run
	NodeTemplate() *sacorev1alpha1.NodeTemplate
	// Run executes the simulation to completion and returns any encountered error. This is a blocking call and callers are
	// expected to manage concurrency and SimulationResult consumption.
	Run(ctx context.Context) error
	// Result returns the latest SimulationResult if the simulation is in ActivityStatusSuccess,
	// or nil if the simulation is in ActivityStatusPending or ActivityStatusRunning
	// or an error if the ActivityStatus is ActivityStatusFailure
	Result() (SimulationResult, error)
}

// SimulationResult contains the results of a completed simulation run.
type SimulationResult struct {
	// Name of the Simulation that produced this result.
	Name string
	// Placement represents the placement information for the Node.
	Placement sacorev1alpha1.NodePlacement
	// ScaledAssignment represents the assignment of the scaled Node for the current run.
	ScaledAssignment *NodePodAssignment
	// OtherAssignments represent the assignment of unscheduled Pods to either an existing Node which is part of the ClusterSnapshot
	// or it is a winning simulated Node from a previous run.
	OtherAssignments []NodePodAssignment
	// UnscheduledPods is the slice of unscheduled pods that remain unscheduled after simulation is completed.
	UnscheduledPods []types.NamespacedName
	// ScaledNode is the simulated scaled node.
	//ScaledNode *corev1.Node
}

// SimulationArgs represents the argument necessary for creating a simulation instance.
type SimulationArgs struct {
	// SchedulerLauncher is used to launch scheduler instances for the simulation.
	SchedulerLauncher SchedulerLauncher
	// View is the MinKAPI view used for the simulation.
	View mkapi.View
	// NodePool is the target node pool for the simulation.
	NodePool *sacorev1alpha1.NodePool
	// GroupRunPassCounter is an atomic counter for tracking simulation group passes.
	GroupRunPassCounter *atomic.Uint32
	// AvailabilityZone is the target availability zone for the simulation.
	AvailabilityZone string
	// NodeTemplateName is the name of the node template to use in the simulation.
	NodeTemplateName string
	// TrackPollInterval is the polling interval for tracking pod scheduling in the view and reconciling simulation state
	TrackPollInterval time.Duration
}

// SimulationCreatorFunc is a factory function for constructing a simulation instance.
// It implements the SimulationCreator interface.
type SimulationCreatorFunc func(name string, args *SimulationArgs) (Simulation, error)

// SimulationCreator is an interface that wraps a method that satisfied SimulationCreatorFunc
type SimulationCreator interface {
	// Create creates a simulation instance with the given name and arguments.
	Create(name string, args *SimulationArgs) (Simulation, error)
}

// Create constructs a new simulation instance with the given name and arguments. Satisfies SimulationCreatorFunc
func (f SimulationCreatorFunc) Create(name string, args *SimulationArgs) (Simulation, error) {
	return f(name, args)
}

// SimulationGroup is a group of simulations at the same priority level (ie a partition of simulations). We attempt to run simulations for the
// given group and get a preferred NodeScore for simulations belonging to a group before moving to the group at the
// next priority.
//
//	Example:1
//		np-a: 1 {nt-a: 1, nt-b: 2, nt-c: 1}
//		np-b: 2 {nt-q: 2, nt-r: 1, nt-s: 1}
//
//		p1: {PoolPriority: 1, NTPriority: 1, nt-a, nt-c}
//		p2: {PoolPriority: 1, NTPriority: 2, nt-b}
//		p3: {PoolPriority: 2, NTPriority: 1, nt-r, nt-s}
//		p4: {PoolPriority: 2, NTPriority: 2, nt-q}
//
//	Example:2
//		np-a: 1 {nt-a: 1, nt-b: 2, nt-c: 1}
//		np-b: 2 {nt-q: 2, nt-r: 1, nt-s: 1}
//		np-c: 1 {nt-x: 2, nt-y: 1}
//
//		g1: {PoolPriority: 1, NTPriority: 1, nt-a, nt-c, nt-y}
//		g2: {PoolPriority: 1, NTPriority: 2, nt-b, nt-x}
//		g3: {PoolPriority: 2, NTPriority: 1, nt-r, nt-s}
//		g4: {PoolPriority: 2, NTPriority: 2, nt-q}
type SimulationGroup interface {
	// Name returns the name of the simulation group.
	Name() string
	// GetKey returns the simulation group key.
	GetKey() SimGroupKey
	// GetSimulations returns all simulations in this group.
	GetSimulations() []Simulation
	// Run executes all simulations in the group and returns the results.
	Run(ctx context.Context) (SimGroupRunResult, error)
}

// SimulationGrouperFunc represents a factory function for grouping Simulation instances into one or more SimulationGroups
type SimulationGrouperFunc func(simulations []Simulation) ([]SimulationGroup, error)

// SimulationGrouper is an interface that wraps a method that satisfies SimulationGrouperFunc
type SimulationGrouper interface {
	Group(simulations []Simulation) ([]SimulationGroup, error)
}

// Group groups Simulation instances into one or more SimulationGroups. Satisfies SimulationGrouperFunc
func (f SimulationGrouperFunc) Group(simulations []Simulation) ([]SimulationGroup, error) {
	return f(simulations)
}

// SimGroupKey represents the key for a SimulationGroup.
type SimGroupKey struct {
	// NodePoolPriority is the priority of the node pool.
	NodePoolPriority int32
	// NodeTemplatePriority is the priority of the node template.
	NodeTemplatePriority int32
}

// String returns a string representation of the SimGroupKey.
func (k SimGroupKey) String() string {
	return fmt.Sprintf("(%d:%d)", k.NodePoolPriority, k.NodeTemplatePriority)
}

// SimGroupRunResult contains the results of running a simulation group.
type SimGroupRunResult struct {
	// Name of the group that produced this result.
	Name string
	// SimulationResults contains the results from all simulations in the group.
	SimulationResults []SimulationResult
	// Key is the simulation group key (partition key)
	Key SimGroupKey
}

// SimGroupScores represents the scoring results for the simulation group after running the NodeScorer against the SimGroupRunResult.
type SimGroupScores struct {
	// WinnerNodeScore is the highest scoring node in the group.
	WinnerNodeScore *NodeScore
	// WinnerNode is the actual node corresponding to the winner score.
	WinnerNode *corev1.Node
	// AllNodeScores contains all computed node scores for the group.
	AllNodeScores []NodeScore
}
